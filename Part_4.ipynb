{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, ID, score):\n",
    "        self.ID = ID\n",
    "        self.score = score\n",
    "        self.adjacents = set()\n",
    "        \n",
    "    def out_degree(self):\n",
    "        return len(self.adjacents)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Override the default Equals behavior\"\"\"\n",
    "        if isinstance(other, self.__class__):\n",
    "            return self.ID == other.ID\n",
    "        return False\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        \"\"\"Define a non-equality test\"\"\"\n",
    "        return not self.__eq__(other)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '(%s, %s)' % (self.ID, self.score)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return self.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self, N, fully_undirected=False):\n",
    "        self.nodes = []\n",
    "        self.N = N\n",
    "        self.fully_undirected = fully_undirected\n",
    "        for i in range(N):\n",
    "            # initializes page rank algorithm.\n",
    "            self.nodes.append(Node(i, 1.0 / float(N)))\n",
    "            \n",
    "    def add_edge(self,i,j):\n",
    "        self.nodes[i].adjacents.add(self.nodes[j])\n",
    "        \n",
    "    def add_edge_und(self,i,j):\n",
    "        self.add_edge(i, j)\n",
    "        self.add_edge(j, i)\n",
    "    \n",
    "    #Little trick for the facebook graph. performance related\n",
    "    def get_nodes_in(self, i):\n",
    "        if self.fully_undirected:\n",
    "            return self.nodes[i].adjacents\n",
    "        else:\n",
    "            return [node for node in self.nodes if self.nodes[i] in node.adjacents]\n",
    "        \n",
    "    def scores(self):\n",
    "        d = {}\n",
    "        for node in self.nodes:\n",
    "            d[node.ID] = node.score\n",
    "        return d\n",
    "    \n",
    "    def update_scores(self, scores_dict):\n",
    "        for ID, score in scores_dict.items():\n",
    "            self.nodes[ID].score = score\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        for i in range(len(self.nodes)):\n",
    "            s+='%s: %s \\n' %(i, self.nodes[i].adjacents)\n",
    "        return s\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_graph(filepath, undirected = False):\n",
    "    with open(filepath) as fileIn:\n",
    "        N = int(fileIn.readline())\n",
    "        g = Graph(N, fully_undirected=undirected)\n",
    "        for line in fileIn:\n",
    "            i, j = (int(s) for s in line.split())\n",
    "            if undirected:\n",
    "                g.add_edge_und(i,j)\n",
    "            else:\n",
    "                g.add_edge(i,j)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PageRank:\n",
    "    def __init__(self, graph, epsilon = 0.00000001, rounds = math.inf, e = 1.0/7):\n",
    "        self.graph = graph\n",
    "        self.epsilon = epsilon\n",
    "        self.rounds = rounds\n",
    "        self.e = e\n",
    "    \n",
    "    # it can terminate based on epsilon change or number of iterations\n",
    "    def converge(self, update_graph=False, scaled_page_rank = True):\n",
    "        _break = False\n",
    "        _round = 0\n",
    "        _scores = self.graph.scores()\n",
    "        while _round < self.rounds and not _break:\n",
    "            _break = True\n",
    "            _round_scores = copy.copy(_scores)\n",
    "            for node in self.graph.nodes:\n",
    "                new_score = self.__page_rank_score(node.ID, _scores)\n",
    "                if scaled_page_rank:\n",
    "                    new_score = (self.e / self.graph.N) + ((1 - self.e) * new_score)\n",
    "                if (abs(new_score - _scores[node.ID])) > self.epsilon:\n",
    "                    _break = False\n",
    "                _round_scores[node.ID] = new_score\n",
    "            _round += 1\n",
    "            _scores = _round_scores\n",
    "        \n",
    "        if update_graph:\n",
    "            self.graph.update_scores(_scores)\n",
    "        return _scores, _round \n",
    "    \n",
    "    def __page_rank_score(self, i, scores):\n",
    "        in_nodes = self.graph.get_nodes_in(i)\n",
    "        scores = [ scores[node.ID] / node.out_degree() for node in in_nodes]\n",
    "        return sum(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### We can use `n` to stop the iterations or it will stop when the change in scores is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fig 11_1_1 - Triangle attached to self loop node z. fig 11.1 (left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard page rank. Iterations: 73. Scores-> {0: 1.4901161193847656e-08, 1: 7.450580596923828e-09, 2: 1.4901161193847656e-08, 3: 0.999999962747097}\n",
      "\n",
      "\n",
      "Scaled page rank. Iterations: 19. Scores-> {0: 0.23333333432674408, 1: 0.18333333358168602, 2: 0.21666666865348816, 3: 0.36666666343808174}\n"
     ]
    }
   ],
   "source": [
    "# read the graph\n",
    "g = read_graph('f_11_1_1.txt')\n",
    "# prepare for page rank\n",
    "page_rank = PageRank(graph = g, e = 1.0/2)\n",
    "# try standard page rank. We will expect a tendency towards 1 for node 3(z) and 0 for the rest\n",
    "scores, niter = page_rank.converge(scaled_page_rank = False)\n",
    "print(\"Standard page rank. Iterations: %s. Scores-> %s\" % (niter, scores))\n",
    "\n",
    "print(\"\\n\")\n",
    "# Now we try scaled page rank.\n",
    "scores, niter = page_rank.converge(scaled_page_rank = True)\n",
    "print(\"Scaled page rank. Iterations: %s. Scores-> %s\" % (niter, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fig 11_1_2 - Triangle attached to two nodes that loop around each other (fig 11.1 right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard page rank. Iterations: 46. Scores-> {0: 1.3938343875251262e-08, 1: 4.646114625083754e-09, 2: 1.3938343875251262e-08, 3: 0.49999998373859866, 4: 0.49999998373859866}\n",
      "\n",
      "\n",
      "Scaled page rank. Iterations: 16. Scores-> {0: 0.18260869783629116, 1: 0.1304347829727152, 2: 0.1652173956725823, 3: 0.2608695617592057, 4: 0.2608695617592057}\n"
     ]
    }
   ],
   "source": [
    "# read the graph\n",
    "g = read_graph('f_11_1_2.txt')\n",
    "# prepare for page rank\n",
    "page_rank = PageRank(graph = g, e = 1.0/2)\n",
    "# try standard page rank. We will expect a tendency towards 0.5 for nodes 3 and 4 and 0 for the rest.\n",
    "scores, niter = page_rank.converge(scaled_page_rank = False)\n",
    "print(\"Standard page rank. Iterations: %s. Scores-> %s\" % (niter, scores))\n",
    "\n",
    "print(\"\\n\")\n",
    "# Now we try scaled page rank.\n",
    "scores, niter = page_rank.converge(scaled_page_rank = True)\n",
    "print(\"Scaled page rank. Iterations: %s. Scores-> %s\" % (niter, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fig 11_2 - two disjoint rectangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard page rank. Iterations: 1. Scores-> {0: 0.16666666666666666, 1: 0.16666666666666666, 2: 0.16666666666666666, 3: 0.16666666666666666, 4: 0.16666666666666666, 5: 0.16666666666666666}\n",
      "Scaled page rank. Iterations: 1. Scores-> {0: 0.16666666666666666, 1: 0.16666666666666666, 2: 0.16666666666666666, 3: 0.16666666666666666, 4: 0.16666666666666666, 5: 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# read the graph\n",
    "g = read_graph('f_11_2.txt')\n",
    "# prepare for page rank\n",
    "page_rank = PageRank(graph = g, e = 1.0/7)\n",
    "# try standard page rank. We will expect a tendency towards 1/6 for all nodes.\n",
    "scores, niter = page_rank.converge(scaled_page_rank = False)\n",
    "print(\"Standard page rank. Iterations: %s. Scores-> %s\" % (niter, scores))\n",
    "\n",
    "# print(\"\\n\")\n",
    "# Now we try scaled page rank. Result should be the same\n",
    "scores, niter = page_rank.converge(scaled_page_rank = True)\n",
    "print(\"Scaled page rank. Iterations: %s. Scores-> %s\" % (niter, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fig 11_3 - single triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard page rank. Iterations: 1. Scores-> {0: 0.3333333333333333, 1: 0.3333333333333333, 2: 0.3333333333333333}\n",
      "Scaled page rank. Iterations: 1. Scores-> {0: 0.3333333333333333, 1: 0.3333333333333333, 2: 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "# read the graph\n",
    "g = read_graph('f_11_3.txt')\n",
    "# prepare for page rank\n",
    "page_rank = PageRank(graph = g, e = 1.0/7)\n",
    "# try standard page rank. We will expect a tendency towards 1/6 for all nodes.\n",
    "scores, niter = page_rank.converge(scaled_page_rank = False)\n",
    "print(\"Standard page rank. Iterations: %s. Scores-> %s\" % (niter, scores))\n",
    "\n",
    "# print(\"\\n\")\n",
    "# Now we try scaled page rank. Result should be the same\n",
    "scores, niter = page_rank.converge(scaled_page_rank = True)\n",
    "print(\"Scaled page rank. Iterations: %s. Scores-> %s\" % (niter, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this is already taken care of by the way the graph is constructed.\n",
    "fb_g = read_graph('facebook_combined.txt', undirected=True)\n",
    "assert(len(fb_g.nodes[0].adjacents) == 347)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are looking at the average score as we iterate through the number of rounds. That should give us an idea of the tendency. We can see that after 4 rounds we converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran for 2 iterations. Avg score for 2 rounds : 0.00024758603614756215 \n",
      "Ran for 3 iterations. Avg score for 3 rounds : 0.0002475860361475614 \n",
      "Ran for 4 iterations. Avg score for 4 rounds : 0.0002475860361475623 \n",
      "Ran for 5 iterations. Avg score for 5 rounds : 0.00024758603614756075 \n",
      "Ran for 6 iterations. Avg score for 6 rounds : 0.0002475860361475613 \n",
      "Ran for 7 iterations. Avg score for 7 rounds : 0.0002475860361475612 \n",
      "Ran for 8 iterations. Avg score for 8 rounds : 0.0002475860361475606 \n",
      "Ran for 9 iterations. Avg score for 9 rounds : 0.00024758603614756183 \n",
      "Ran for 10 iterations. Avg score for 10 rounds : 0.00024758603614756134 \n",
      "Ran for 11 iterations. Avg score for 11 rounds : 0.0002475860361475615 \n",
      "Ran for 12 iterations. Avg score for 12 rounds : 0.00024758603614756205 \n",
      "Ran for 13 iterations. Avg score for 13 rounds : 0.0002475860361475616 \n",
      "Ran for 14 iterations. Avg score for 14 rounds : 0.00024758603614756205 \n",
      "Ran for 15 iterations. Avg score for 15 rounds : 0.0002475860361475613 \n",
      "Ran for 16 iterations. Avg score for 16 rounds : 0.000247586036147561 \n",
      "Ran for 17 iterations. Avg score for 17 rounds : 0.000247586036147561 \n",
      "Ran for 18 iterations. Avg score for 18 rounds : 0.000247586036147561 \n",
      "Ran for 19 iterations. Avg score for 19 rounds : 0.0002475860361475608 \n"
     ]
    }
   ],
   "source": [
    "# read the graph\n",
    "fb_g = read_graph('facebook_combined.txt', undirected=True) \n",
    "for rounds in range(2, 20, 1):\n",
    "        page_rank = PageRank(graph = fb_g, e = 1.0/7, rounds = rounds)\n",
    "        scores, niter = page_rank.converge()\n",
    "        print('Ran for %s iterations. Avg score for %s rounds : %s ' % (niter, rounds, sum(scores.values()) / len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg score for 2 rounds : 0.0002567400396618219 \n",
      "Avg score for 4 rounds : 0.00025254916998110517 \n",
      "Avg score for 6 rounds : 0.00025254916998110517 \n",
      "Avg score for 8 rounds : 0.00025254916998110517 \n",
      "Avg score for 10 rounds : 0.00025254916998110517 \n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 12, 2):\n",
    "    \n",
    "    fb_g.page_rank(rounds = i)\n",
    "    scores = fb_g.scores()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And without rounds stop condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran for 48 iterations. Avg score: 0.0002475860361475611 \n"
     ]
    }
   ],
   "source": [
    "# read the graph\n",
    "fb_g = read_graph('facebook_combined.txt', undirected=True) \n",
    "page_rank = PageRank(graph = fb_g, e = 1.0/7)\n",
    "scores, niter = page_rank.converge()\n",
    "print('Ran for %s iterations. Avg score: %s ' % (niter,sum(scores.values()) / len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that there is a big gap between the maximum score and the minimum score. That can mean that there are nodes that are the neuralgic centers of the networks, or influencers. We could use these nodes if we were to try and have full cascade in a contagion scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3437, 0.007569615371870586)\n",
      "(2079, 3.9677335824609284e-05)\n"
     ]
    }
   ],
   "source": [
    "# There doesn't seem to be a particular bucketing in scores.\n",
    "print(max(scores.items(), key=operator.itemgetter(1)))\n",
    "print(min(scores.items(), key=operator.itemgetter(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d\n",
    "#### This is definitely a measure of popularity. High scoring nodes are highly connected, which implies that they are very relevant or popular. However, there is a difference between been highly connected and highly influencial: one person can have many friends but  have very little influence on these friends."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
